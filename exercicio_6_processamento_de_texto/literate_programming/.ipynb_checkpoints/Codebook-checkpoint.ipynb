{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Titulo:** Exercício 6 - Aprendizado de Máquina em textos <br>\n",
    "**Autor:** Juan Sebastián Beleño Díaz <br>\n",
    "**Data:** 15 de Novembro de 2016 <br>\n",
    "\n",
    "## Introdução\n",
    "Neste trabalho é feito o pipeline completo para a classificação de textos: processamento dos textos(conversao de caracteres maiusculos para minusculos, remoçao de pontuaçao, remoçao de stop words, steming dos termos, etc.), processamento de *Bag of Words* e *Term-Frequency Matrix*, classificação nas matrizes esparsas usando Naïve Bayes e Regressão Logística, finalmente, classificação na *Term-Frequency Matrix* com PCA usando SVM e Random Forest.\n",
    "\n",
    "## Dados\n",
    "Os arquivos utilizados neste trabalho são os [textos](http://www.ic.unicamp.br/~wainer/cursos/2s2016/ml/ex6/file-sk.zip) e as [classes dos textos](http://www.ic.unicamp.br/~wainer/cursos/2s2016/ml/ex6/category.tab). O arquivo de *textos* é um arquivo zip que contem um conjunto de pastas com o nome das categorias e dentro delas estão os arquivos de cada categoria. O arquivo *classes dos textos* contem duas colunas: a primerira delas é o nome do arquivo e a segunda é a categoria à qual pertence o arquivo.\n",
    "\n",
    "## Preparação dos dados\n",
    "Antes de começar trablahar com os dados é preciso incluir as dependecias do projeto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "\n",
    "from nltk.stem.snowball import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem muitas maneiras de abrir os arquivos e obter os dados, mas neste caso foi usado *pandas* para obter os dataframes diretamente desde a URL. Além disso, foi utilizado *urllib* para obter o arquivo zip e *zipfile* para descompactar as pastas do arquivo zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# URLs with data\n",
    "url_zip_data = 'http://www.ic.unicamp.br/%7Ewainer/cursos/2s2016/ml/ex6/file-sk.zip'\n",
    "url_categories = 'http://www.ic.unicamp.br/%7Ewainer/cursos/2s2016/ml/ex6/category.tab'\n",
    "\n",
    "# Local path for different files and directories\n",
    "filepath_zip = '../assets/file-sk.zip'\n",
    "dirpath_zip = '../assets'\n",
    "directories = {'Apps': '../assets/filesk/Apps/',\n",
    "               'Enterprise': '../assets/filesk/Enterprise/',\n",
    "               'Gadgets': '../assets/filesk/Gadgets/',\n",
    "               'Social': '../assets/filesk/Social/',\n",
    "               'Startups': '../assets/filesk/Startups/'}\n",
    "# dirpath_zip = '../assets/file-sk'\n",
    "\n",
    "# Upload the zip file\n",
    "urllib.request.urlretrieve(url_zip_data, filepath_zip)\n",
    "\n",
    "# Creating the directory where I'll put the uncompressed files\n",
    "# if not os.path.exists(dirpath_zip):\n",
    "#    os.makedirs(dirpath_zip)\n",
    "\n",
    "# Uncompress the zip files in a directory\n",
    "zip_ref = zipfile.ZipFile(filepath_zip, 'r')\n",
    "zip_ref.extractall(dirpath_zip)\n",
    "zip_ref.close()\n",
    "\n",
    "# Extracting the categories for each file\n",
    "df_categories = pd.read_csv(url_categories, \n",
    "                            header= None, \n",
    "                            names= ['file', 'category'],\n",
    "                            skiprows = [0],\n",
    "                            delimiter = \" \" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1 - Processamento de textos\n",
    "Para facilitar as tarefas de classificação foi preciso primeiro processar os dados para diminuir a dimensionalidade e escolher atributos relevantes usando: conversao de caracteres maiusculos para minusculos, remoçao de pontuaçao, remoçao de stop words, steming dos termos, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punctuation = ['.', ',', ';', ':', \" ' \", \" 's \", '?', \n",
    "               '\"', '”', '“', '’s', '—', '/'\n",
    "               '(', ')', '[', ']', '1', '2', '3', \n",
    "               '4', '5', '6', '7', '8', '9', '0',\n",
    "               '$', '%', '–', '•', '~']\n",
    "               \n",
    "stop_words = [\" a \", \" about \", \" above \", \" above \", \" across \", \" after \", \" afterwards \",\n",
    "              \" again \", \" against \", \" all \", \" almost \", \" alone \", \" along \", \" already \",\n",
    "              \" also \",\"although \",\"always \",\"am \",\"among \", \" amongst \", \" amoungst \", \n",
    "              \" amount \",  \" an \", \" and \", \" another \", \" any \",\"anyhow \",\"anyone \",\n",
    "              \" anything \",\"anyway \", \" anywhere \", \" are \", \" around \", \" as \",  \" at \", \n",
    "              \" back \",\"be \",\"became \", \" because \",\"become \",\"becomes \", \" becoming \", \n",
    "              \" been \", \" before \", \" beforehand \", \" behind \", \" being \", \" below \", \n",
    "              \" beside \", \" besides \", \" between \", \" beyond \", \" bill \", \" both \", \n",
    "              \" bottom \",\"but \", \" by \", \" call \", \" can \", \" cannot \", \" cant \", \" co \", \n",
    "              \" con \", \" could \", \" couldnt \", \" cry \", \" de \", \" describe \", \" detail \", \n",
    "              \" do \", \" done \", \" down \", \" due \", \" during \", \" each \", \" eg \", \" eight \", \n",
    "              \" either \", \" eleven \",\"else \", \" elsewhere \", \" empty \", \" enough \", \" etc \", \n",
    "              \" even \", \" ever \", \" every \", \" everyone \", \" everything \", \" everywhere \", \n",
    "              \" except \", \" few \", \" fifteen \", \" fify \", \" fill \", \" find \", \" fire \", \n",
    "              \" first \", \" five \", \" for \", \" former \", \" formerly \", \" forty \", \" found \", \n",
    "              \" four \", \" from \", \" front \", \" full \", \" further \", \" get \", \" give \", \" go \", \n",
    "              \" had \", \" has \", \" hasnt \", \" have \", \" he \", \" hence \", \" her \", \" here \", \n",
    "              \" hereafter \", \" hereby \", \" herein \", \" hereupon \", \" hers \", \" herself \", \n",
    "              \" him \", \" himself \", \" his \", \" how \", \" however \", \" hundred \", \" ie \", \" if \", \n",
    "              \" in \", \" inc \", \" indeed \", \" interest \", \" into \", \" is \", \" it \", \" its \", \n",
    "              \" itself \", \" keep \", \" last \", \" latter \", \" latterly \", \" least \", \" less \", \n",
    "              \" ltd \", \" made \", \" many \", \" may \", \" me \", \" meanwhile \", \" might \", \" mill \", \n",
    "              \" mine \", \" more \", \" moreover \", \" most \", \" mostly \", \" move \", \" much \", \n",
    "              \" must \", \" my \", \" myself \", \" name \", \" namely \", \" neither \", \" never \", \n",
    "              \" nevertheless \", \" next \", \" nine \", \" no \", \" nobody \", \" none \", \" noone \", \n",
    "              \" nor \", \" not \", \" nothing \", \" now \", \" nowhere \", \" of \", \" off \", \" often \", \n",
    "              \" on \", \" once \", \" one \", \" only \", \" onto \", \" or \", \" other \", \" others \", \n",
    "              \" otherwise \", \" our \", \" ours \", \" ourselves \", \" out \", \" over \", \" own \",\n",
    "              \" part \", \" per \", \" perhaps \", \" please \", \" put \", \" rather \", \" re \", \" same \", \n",
    "              \" see \", \" seem \", \" seemed \", \" seeming \", \" seems \", \" serious \", \" several \", \n",
    "              \" she \", \" should \", \" show \", \" side \", \" since \", \" sincere \", \" six \", \" sixty \", \n",
    "              \" so \", \" some \", \" somehow \", \" someone \", \" something \", \" sometime \", \n",
    "              \" sometimes \", \" somewhere \", \" still \", \" such \", \" system \", \" take \", \" ten \", \n",
    "              \" than \", \" that \", \" the \", \" their \", \" them \", \" themselves \", \" then \", \n",
    "              \" thence \", \" there \", \" thereafter \", \" thereby \", \" therefore \", \" therein \", \n",
    "              \" thereupon \", \" these \", \" they \", \" thickv \", \" thin \", \" third \", \" this \", \n",
    "              \" those \", \" though \", \" three \", \" through \", \" throughout \", \" thru \", \n",
    "              \" thus \", \" to \", \" together \", \" too \", \" top \", \" toward \", \" towards \", \n",
    "              \" twelve \", \" twenty \", \" two \", \" un \", \" under \", \" until \", \" up \", \" upon \", \n",
    "              \" us \", \" very \", \" via \", \" was \", \" we \", \" well \", \" were \", \" what \", \n",
    "              \" whatever \", \" when \", \" whence \", \" whenever \", \" where \", \" whereafter \", \n",
    "              \" whereas \", \" whereby \", \" wherein \", \" whereupon \", \" wherever \", \" whether \", \n",
    "              \" which \", \" while \", \" whither \", \" who \", \" whoever \", \" whole \", \" whom \", \n",
    "              \" whose \", \" why \", \" will \", \" with \", \" within \", \" without \", \" would \", \n",
    "              \" yet \", \" you \", \" your \", \" yours \", \" yourself \", \" yourselves \", \" the \"]\n",
    "\n",
    "# Iterate over the directories to clean the dataset\n",
    "for directory in directories:\n",
    "    # for filename in os.listdir(os.getcwd()):\n",
    "    for filename in np.ravel(df_categories['file'].loc[df_categories['category'] == directory]):\n",
    "\n",
    "        # Setting up the relative path for the file\n",
    "        filename = directories[directory] + str(filename) + '.txt'\n",
    "        \n",
    "        # Open the file and read the content\n",
    "        with open(filename, \"r\") as inputFile:\n",
    "            content = inputFile.read()\n",
    "            \n",
    "        # Open the file in writing mode\n",
    "        with open(filename, \"w\") as outputFile:\n",
    "            \n",
    "            # Transform the content to lowercase\n",
    "            content = content.lower()\n",
    "            \n",
    "            # Remove punctuation\n",
    "            for char in punctuation:\n",
    "                content = content.replace(char, '')\n",
    "                \n",
    "            # special puntuation\n",
    "            content = content.replace('’re', ' are')\n",
    "            content = content.replace('n’t', ' not')\n",
    "            content = content.replace('s’', 's')\n",
    "            content = content.replace('-', ' ')\n",
    "            \n",
    "            # Remove stop words\n",
    "            # content = [w for w in content if not w in stopwords.words(\"english\")]\n",
    "            for stop_word in stop_words:\n",
    "                content = content.replace(stop_word, ' ')\n",
    "                \n",
    "                \n",
    "            # Steming\n",
    "            stemmer = PorterStemmer()\n",
    "            words = word_tokenize(content)\n",
    "            stem_words = [stemmer.stem(w) for w in words]\n",
    "            content = \" \".join(stem_words)\n",
    "            \n",
    "            # write the preprocessed content\n",
    "            outputFile.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É preciso conhecer as dimensões das matrizes *Bag of Words* e *Term-Frequency Matrix* antes de trabalhar com elas para saber se a memória RAM é suficente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The words in all documents\n",
    "word_list = []\n",
    "\n",
    "# Iterate over the directories to find the words in all the documents\n",
    "for directory in directories:\n",
    "    for filename in np.ravel(df_categories['file'].loc[df_categories['category'] == directory]):\n",
    "\n",
    "        # Setting up the relative path for the file\n",
    "        filename = directories[directory] + str(filename) + '.txt'\n",
    "        \n",
    "        # Open the file and read the content\n",
    "        with open(filename, \"r\") as inputFile:\n",
    "            content = inputFile.read()\n",
    "            \n",
    "            # Getting the word of the text in array format\n",
    "            words = content.split()\n",
    "            \n",
    "            word_list.extend(words)\n",
    "            word_list = list(set(word_list))\n",
    "            \n",
    "print(\"Número de palavras: \", len(word_list)) # 30,940\n",
    "print(\"Número de documentos: \", df_categories.shape[0]) # 5,000\n",
    "print(\"----------------------------------------\")\n",
    "\n",
    "# 2 arrays, each one with 30,940 columns and 5,000 rows\n",
    "# Storing data in int32 => 30,940 * 5,000 * 2 * 4 bytes ~ 1.2376 GB RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consequentemente, calculamos *Bag of Words* e *Term-Frequency Matrix*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bags of words and term frequency matrix\n",
    "bag_of_words = np.zeros(shape=(df_categories.shape[0], len(word_list)))\n",
    "tf_matrix = np.zeros(shape=(df_categories.shape[0], len(word_list)))\n",
    "\n",
    "# Iterate over the directories to do bag of words and TF matrix\n",
    "for directory in directories:\n",
    "    for filename in np.ravel(df_categories['file'].loc[df_categories['category'] == directory]):\n",
    "\n",
    "        # Setting up the relative path for the file\n",
    "        file_name = directories[directory] + str(filename) + '.txt'\n",
    "        \n",
    "        # Open the file and read the content\n",
    "        with open(file_name, \"r\") as inputFile:\n",
    "            content = inputFile.read()\n",
    "            \n",
    "            # Getting the word of the text in array format\n",
    "            words = content.split()\n",
    "            \n",
    "            for w in words:\n",
    "                word_index = word_list.index(w)\n",
    "                bag_of_words[filename - 1][word_index] = 1\n",
    "                tf_matrix[filename - 1][word_index] = tf_matrix[filename - 1][word_index] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2 - classificador multiclasse na matriz termo-documento original\n",
    "É separado aleatoriamente os conjunto de treino(4000 textos) e de teste(1000 textos) para cada matriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classes dataframe\n",
    "df_classes = np.ravel(df_categories['category'])\n",
    "\n",
    "# Split the dataset 4000 for training and 1000 for testing randomically\n",
    "# We need at least 1.2376 GB RAM more to split the data\n",
    "\n",
    "# Split Bag of Words in test and train data\n",
    "BW_train, BW_test, BW_categories_train, BW_categories_test = train_test_split(\n",
    "    bag_of_words, df_classes, test_size=0.2, random_state=1992)\n",
    "    \n",
    "# Split Term Frequency Matrix in test and train data\n",
    "TF_train, TF_test, TF_categories_train, TF_categories_test = train_test_split(\n",
    "    tf_matrix, df_classes, test_size=0.2, random_state=1992)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executamos o classificador Naïve Bayes nas matrizes de *Bags of words* and *term frequency*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Naive Bayes on the Bag of Words\n",
    "clf_naive_bayes = MultinomialNB()\n",
    "clf_naive_bayes.fit(BW_train, BW_categories_train)\n",
    "score_nb_bw = clf_naive_bayes.score(BW_test, BW_categories_test)\n",
    "\n",
    "# Naive Bayes on the Term Frequency Matrix \n",
    "# Reusing the classifier to optimize memory\n",
    "clf_naive_bayes = MultinomialNB()\n",
    "clf_naive_bayes.fit(TF_train, TF_categories_train)\n",
    "score_nb_tf = clf_naive_bayes.score(TF_test, TF_categories_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executamos o classificador de Regressão Logística nas matrizes de *Bags of words* and *term frequency*. Usando um valor de C=100000 para evitar que haja regularização e usamos paralelização para melhorar os tempos de execução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Changing C value in Logistic Regression to prevent regularization\n",
    "param_C=10000\n",
    "\n",
    "# Improving the performance using parallelization\n",
    "n_jobs = 3\n",
    "\n",
    "# Logistic Regression on the Bag of Words\n",
    "clf_lr = LogisticRegression(C = param_C, n_jobs = n_jobs)\n",
    "clf_lr.fit(BW_train, BW_categories_train)\n",
    "score_lr_bw = clf_lr.score(BW_test, BW_categories_test)\n",
    "\n",
    "# Logistic Regression on the Term Frequency Matrix \n",
    "# Reusing the classifier to optimize memory\n",
    "clf_lr = LogisticRegression(C = param_C, n_jobs = n_jobs)\n",
    "clf_lr.fit(TF_train, TF_categories_train)\n",
    "score_lr_tf = clf_lr.score(TF_test, TF_categories_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, são apresentados os resultados destes dois classificadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Results\n",
    "print('Acurácia de Naive Bayes em Bag of Words: ', score_nb_bw)\n",
    "print('Acurácia de Naive Bayes em Term Frequency Matrix: ', score_nb_tf)\n",
    "print('Acurácia de Logistic Regression em Bag of Words: ', score_lr_bw)\n",
    "print('Acurácia de Logistic Regression em Term Frequency Matrix: ', score_lr_tf)\n",
    "print('----------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3 - classificador multiclasse na matriz termo-documento reduzida\n",
    "Fizemos uma redução de dimensionalidade usando PCA sobre a matriz de *term frequency*, mantendo o 99% da variância."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "variance_percentage_pca = 0.99\n",
    "\n",
    "# PCA in Term Frequency matrix\n",
    "pca = PCA(n_components = variance_percentage_pca)\n",
    "pca.fit(TF_train)\n",
    "params_reduced_train = pca.transform(TF_train)\n",
    "params_reduced_test = pca.transform(TF_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executamos o classificador SVM sobre a matriz reduzida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SVM Classifier with RBF kernel\n",
    "clf_svm = SVC()\n",
    "clf_svm.fit(params_reduced_train, TF_categories_train)\n",
    "score_svm = clf_svm.score(params_reduced_test, TF_categories_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executamos o classificador Random Forest sobre a matriz reducidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "clf_rf = RandomForestClassifier()\n",
    "clf_rf.fit(params_reduced_train, TF_categories_train)\n",
    "score_rf = clf_rf.score(params_reduced_test, TF_categories_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, são aprensentados os resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Classificação em TF Matrix com dados de dimensionalidade reduzida por PCA')\n",
    "print('Acurácia SVM: ', score_svm)\n",
    "print('Acurácia Random Forest: ', score_rf)\n",
    "print('-------------------------------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
