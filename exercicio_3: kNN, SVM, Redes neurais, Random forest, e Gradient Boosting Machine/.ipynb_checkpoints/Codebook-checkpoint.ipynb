{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Titulo:** Exercício 3 - kNN, SVM, Redes Neurais, Random Forest e Gradient Boosting <br>\n",
    "**Autor:** Juan Sebastián Beleño Díaz <br>\n",
    "**Data:** 21 de Outubro de 2016 <br>\n",
    "\n",
    "## Introdução\n",
    "Neste trabalho é feita uma comparação na precisão de diferentes classificadores(kNN, SVM, Redes Neurais, Random Forest e Gradient Boosting). Todos os classificadores usam uma validação cruzada externa de 5 folds para achar a média da precisão e uma validação cruzada interna de 3 folds para escolher os hiperparâmetros.\n",
    "\n",
    "## Dados\n",
    "Os arquivos usados neste trabalho são [secom.data](https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data) e [secom_labels.data](https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data) que pertencem ao conjunto de dados [SECOM](https://archive.ics.uci.edu/ml/datasets/SECOM). O conjunto de dados do SECOM foram dados coletados dum processo de fabricação de semicondutores complexo. O arquivo *secom.data* contem os dados principais usados neste trabalho. O arquivo *secom_labels.data* na primeira coluna contem as classes dos dados de *secom.data*.\n",
    "\n",
    "## Preparação dos dados\n",
    "Antes de começar trablahar com os dados é preciso incluir as dependecias do projeto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem muitas maneiras de abrir os arquivos e obter os dados, mas neste caso foi usado *pandas* para obter o dataframe diretamente desde a URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the URIs with raw data\n",
    "url_parameters = 'https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data'\n",
    "url_results = 'https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data'\n",
    "\n",
    "# Reading the files with the raw data\n",
    "df_parameters = pd.read_csv(url_parameters, header = 0, delimiter = \" \")\n",
    "df_results = pd.read_csv(url_results, header = 0, delimiter = \" \")\n",
    "\n",
    "# Getting classes from result\n",
    "df_classes = df_results.iloc[:, 0:1]\n",
    "df_classes = np.ravel(df_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No código embaixo foram declaradas as variáveis que seram usadas pelos classificadores, incluindo os possíveis hiperparâmetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of columns and rows in the raw data\n",
    "n_columns = df_parameters.shape[1]\n",
    "n_rows = df_parameters.shape[0]\n",
    "\n",
    "# Precision mean for all models\n",
    "knn_precision = 0 \n",
    "svm_precision = 0\n",
    "neural_net_precision = 0\n",
    "random_forest_precision = 0\n",
    "gbm_precision = 0\n",
    "\n",
    "# Folds variables\n",
    "n_external_folds = 5\n",
    "n_internal_folds = 3\n",
    "\n",
    "# 80% of variance in the PCA\n",
    "variance_percentage_pca = 0.8\n",
    "n_components_pca = 0\n",
    "\n",
    "# k values for kNN\n",
    "knn_parameters = {'n_neighbors':[1, 5, 11, 15, 21, 25]}\n",
    "\n",
    "# parameters for SVM\n",
    "svm_parameters = {'kernel':['rbf'], 'C':[2**(-5), 2**(0), 2**(5), 2**(10)], 'gamma':[2**(-15), 2**(-10), 2**(-5), 2**(0), 2**(5)]}\n",
    "\n",
    "# Number of neurons in the hidden layer for Neural nets\n",
    "neural_nets_parameters = {'hidden_layer_sizes':[10, 20, 30, 40]}\n",
    "\n",
    "# Random Forest parameters\n",
    "random_forest_parameters = {'max_features':[10, 15, 20, 25], 'n_estimators':[100, 200, 300, 400]}\n",
    "\n",
    "# Parameters for Gradient Boosting Machine\n",
    "gbm_parameters ={'learning_rate':[0.1, 0.05], 'max_depth':[5], 'n_estimators':[30, 70, 100]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definição de classificadores\n",
    "Depois de preparar os dados, já podemos empezar declarar os classificadores em funções para melhorar a modularidade do código.\n",
    "\n",
    "## Classificador kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_precision_kNN_PCA(parameters, vp_pca, train_params, test_params, train_classes, test_classes, n_folds):\n",
    "    \n",
    "    # Applying the PCA keeping the variance over vp_pca %\n",
    "    pca = PCA(n_components = vp_pca)\n",
    "    pca.fit(train_params)\n",
    "    params_reduced_train = pca.transform(train_params)\n",
    "    params_reduced_test = pca.transform(test_params)\n",
    "        \n",
    "    # GridSearch over the kNN parameters using a 3 KFold\n",
    "    # The cv parameter is for Cross-validation\n",
    "    # We find the hyperparameters here\n",
    "    knn = KNeighborsClassifier()\n",
    "    clf_knn = GridSearchCV(knn, parameters, cv=n_folds)\n",
    "    clf_knn.fit(params_reduced_train, train_classes)\n",
    "    \n",
    "    # Getting the best hyperparameters\n",
    "    knn_best_hyperparams = clf_knn.best_params_\n",
    "    \n",
    "    # Create the best kNN model\n",
    "    knn_tuned = KNeighborsClassifier(n_neighbors=knn_best_hyperparams['n_neighbors'])\n",
    "    knn_tuned.fit(params_reduced_train, train_classes)\n",
    "    \n",
    "    # Get the precision of the model\n",
    "    knn_tuned_score = knn_tuned.score(params_reduced_test, test_classes)\n",
    "    \n",
    "    return knn_tuned_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificador SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_precision_svm(parameters, train_params, test_params, train_classes, test_classes, n_folds):\n",
    "    \n",
    "    # GridSearch over the SVM parameters using a 3 KFold\n",
    "    # The cv parameter is for Cross-validation\n",
    "    # We find the hyperparameters here\n",
    "    svm = SVC()\n",
    "    clf_svm = GridSearchCV(svm, parameters, cv=n_folds)\n",
    "    clf_svm.fit(train_params, train_classes)\n",
    "    \n",
    "    # Getting the best hyperparameters\n",
    "    svm_best_hyperparams = clf_svm.best_params_    \n",
    "    \n",
    "    # Create the best SVM model\n",
    "    svm_tuned = SVC(C = svm_best_hyperparams['C'], kernel = svm_best_hyperparams['kernel'], gamma = svm_best_hyperparams['gamma'])\n",
    "    svm_tuned.fit(train_params, train_classes)\n",
    "    \n",
    "    # Getting the model precision\n",
    "    svm_tuned_score = svm_tuned.score(test_params, test_classes)\n",
    "    \n",
    "    return svm_tuned_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificador Redes Neurais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_precision_neural_nets(parameters, train_params, test_params, train_classes, test_classes, n_folds):\n",
    "    \n",
    "    # GridSearch over the Neural Nets parameters using a 3 KFold\n",
    "    # The cv parameter is for Cross-validation\n",
    "    # We find the hyperparameters here\n",
    "    nn = MLPClassifier()\n",
    "    clf_nn = GridSearchCV(nn, parameters, cv=n_folds)\n",
    "    clf_nn.fit(train_params, train_classes)\n",
    "    \n",
    "    # Getting the best hyperparameters\n",
    "    nn_best_hyperparams = clf_nn.best_params_    \n",
    "    \n",
    "    # Create the best Neural Net model\n",
    "    nn_tuned = MLPClassifier(hidden_layer_sizes = nn_best_hyperparams['hidden_layer_sizes'])\n",
    "    nn_tuned.fit(train_params, train_classes)\n",
    "    \n",
    "    # Getting the model precision\n",
    "    nn_tuned_score = nn_tuned.score(test_params, test_classes)\n",
    "    \n",
    "    return nn_tuned_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificador Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_precision_random_forest(parameters, train_params, test_params, train_classes, test_classes, n_folds):\n",
    "    \n",
    "    # GridSearch over the Random Forest parameters using a 3 KFold\n",
    "    # The cv parameter is for Cross-validation\n",
    "    # We find the hyperparameters here\n",
    "    rf = RandomForestClassifier()\n",
    "    clf_rf = GridSearchCV(rf, parameters, cv=n_folds)\n",
    "    clf_rf.fit(train_params, train_classes)\n",
    "    \n",
    "    # Getting the best hyperparameters\n",
    "    rf_best_hyperparams = clf_rf.best_params_    \n",
    "    \n",
    "    # Create the best Random Forest model\n",
    "    rf_tuned = RandomForestClassifier(max_features = rf_best_hyperparams['max_features'], n_estimators = rf_best_hyperparams['n_estimators'])\n",
    "    rf_tuned.fit(train_params, train_classes)\n",
    "    \n",
    "    # Getting the model precision\n",
    "    rf_tuned_score = rf_tuned.score(test_params, test_classes)\n",
    "    \n",
    "    return rf_tuned_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificador GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_precision_gbm(parameters, train_params, test_params, train_classes, test_classes, n_folds):\n",
    "    \n",
    "    # GridSearch over the Grid Boosting parameters using a 3 KFold\n",
    "    # The cv parameter is for Cross-validation\n",
    "    # We find the hyperparameters here\n",
    "    gbm = GradientBoostingClassifier()\n",
    "    clf_gbm = GridSearchCV(gbm, parameters, cv=n_folds)\n",
    "    clf_gbm.fit(train_params, train_classes)\n",
    "    \n",
    "    # Getting the best hyperparameters\n",
    "    gbm_best_hyperparams = clf_gbm.best_params_    \n",
    "    \n",
    "    # Create the best Grid Boosting model\n",
    "    gbm_tuned = GradientBoostingClassifier(learning_rate = gbm_best_hyperparams['learning_rate'], max_depth = gbm_best_hyperparams['max_depth'], n_estimators = gbm_best_hyperparams['n_estimators'])\n",
    "    gbm_tuned.fit(train_params, train_classes)\n",
    "    \n",
    "    # Getting the model precision\n",
    "    gbm_tuned_score = gbm_tuned.score(test_params, test_classes)\n",
    "    \n",
    "    return gbm_tuned_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento dos classificadores\n",
    "Neste ponto o código faz uma validaçaõ cruzada externa de 5 folds, uma imputação de dados usando a média da colunas, normaliza os dados e faz uma validação cruzada interna de 3 folds sobre cada classificador para achar a precisão de cada um deles. As precisões são somadas em variaveis para depois calcular a média dos classificadores segundo a validação cruzada externa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Juan/anaconda3/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------- Here goes the magic -----------------------------\n",
    "\n",
    "# Define the external K-Fold Stratified\n",
    "external_skf = StratifiedKFold(n_splits = n_external_folds)\n",
    "external_skf.get_n_splits(df_parameters, df_classes)\n",
    "\n",
    "# Iterate over external data\n",
    "for external_train_index, external_test_index in external_skf.split(df_parameters, df_classes):\n",
    "    \n",
    "    # Split the external training set and the external test set\n",
    "    external_params_train = df_parameters.iloc[external_train_index, :] \n",
    "    external_classes_train = df_classes[external_train_index] \n",
    "    external_params_test = df_parameters.iloc[external_test_index, :]\n",
    "    external_classes_test = df_classes[external_test_index]\n",
    "    \n",
    "    # *********************** Imputation of data ****************************\n",
    "    imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "    imp.fit(external_params_train)\n",
    "\n",
    "    # Appling the imputation    \n",
    "    imp_external_params_train = imp.transform(external_params_train)\n",
    "    imp_external_params_test = imp.transform(external_params_test)\n",
    "    \n",
    "    # Scaling the data\n",
    "    scaler = StandardScaler().fit(imp_external_params_train)\n",
    "    scaled_external_params_train = scaler.transform(imp_external_params_train)\n",
    "    scaled_external_params_test = scaler.transform(imp_external_params_test)\n",
    "    \n",
    "    # Cleaning NaN for bad scaling\n",
    "    # clean_external_params_train = np.nan_to_num(scaled_external_params_train)\n",
    "    # clean_external_params_test = np.nan_to_num(scaled_external_params_test)\n",
    "     \n",
    "    \n",
    "    # Getting the kNN precision for this fold keeping PCA with 80% of the variance and internal CV 3-Fold\n",
    "    knn_score = get_precision_kNN_PCA(knn_parameters, variance_percentage_pca, scaled_external_params_train, scaled_external_params_test, external_classes_train, external_classes_test, n_internal_folds)\n",
    "    # 0.929769178069\n",
    "    \n",
    "    # Getting the precision of SVM with kernel RBF using a 3-Fold internal CV\n",
    "    svm_score = get_precision_svm(svm_parameters, scaled_external_params_train, scaled_external_params_test, external_classes_train, external_classes_test, n_internal_folds)\n",
    "    # 0.93359083412    \n",
    "    \n",
    "    # Getting the precision of neural nets\n",
    "    neural_net_score = get_precision_neural_nets(neural_nets_parameters, scaled_external_params_train, scaled_external_params_test, external_classes_train, external_classes_test, n_internal_folds)\n",
    "    # 0.853934283295\n",
    "\n",
    "    # Getting the precision of random forest\n",
    "    random_forest_score = get_precision_random_forest(random_forest_parameters, scaled_external_params_train, scaled_external_params_test, external_classes_train, external_classes_test, n_internal_folds)\n",
    "    # 0.93359083412\n",
    "\n",
    "    # Getting the precision of gradient boosting\n",
    "    gbm_score = get_precision_gbm(gbm_parameters, scaled_external_params_train, scaled_external_params_test, external_classes_train, external_classes_test, n_internal_folds)\n",
    "    # 0.839945990058\n",
    "    \n",
    "    # Stacking the precision\n",
    "    knn_precision = knn_precision + knn_score\n",
    "    svm_precision = svm_precision + svm_score\n",
    "    neural_net_precision = neural_net_precision + neural_net_score\n",
    "    random_forest_precision = random_forest_precision + random_forest_score\n",
    "    gbm_precision = gbm_precision + gbm_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_precision = knn_precision/n_external_folds\n",
    "svm_precision = svm_precision/n_external_folds\n",
    "neural_net_precision = neural_net_precision/n_external_folds\n",
    "random_forest_precision = random_forest_precision/n_external_folds\n",
    "gbm_precision = gbm_precision/n_external_folds\n",
    "\n",
    "print('Accuracy kNN: ', knn_precision)\n",
    "print('Accuracy SVM: ', svm_precision)\n",
    "print('Accuracy Neural Networks: ', neural_net_precision)\n",
    "print('Accuracy Random Forest: ', random_forest_precision)\n",
    "print('Accuracy Gradient Boosting: ', gbm_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusões\n",
    "O melhores classificadores para o conjunto de dados SECOM foram SVM e Random Forest com uma acurácia de **93.359%**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
